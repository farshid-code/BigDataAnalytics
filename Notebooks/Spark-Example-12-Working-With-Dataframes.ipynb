{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.sql.functions import collect_set\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "from pyspark.ml.linalg import SparseVector, DenseVector\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n",
      "+---+-------------+\n",
      "| id|     features|\n",
      "+---+-------------+\n",
      "|  1|[2.0,2.0,3.0]|\n",
      "|  1|[2.0,3.0,3.0]|\n",
      "|  2|[3.0,2.0,3.0]|\n",
      "|  2|[3.0,3.0,3.0]|\n",
      "+---+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let us use Vector Dense Matrix \n",
    "data2 = [(1, Vectors.dense([2.0, 2.0, 3.0]),),\n",
    "         (1, Vectors.dense([2.0, 3.0, 3.0]),),\n",
    "         (2, Vectors.dense([3.0, 2.0, 3.0]),),\n",
    "         (2, Vectors.dense([3.0, 3.0, 3.0]),)]\n",
    "\n",
    "df = spark.createDataFrame(data2, [\"id\", \"features\"])\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+---------------+\n",
      "| id|     features| features_array|\n",
      "+---+-------------+---------------+\n",
      "|  1|[2.0,2.0,3.0]|[2.0, 2.0, 3.0]|\n",
      "|  1|[2.0,3.0,3.0]|[2.0, 3.0, 3.0]|\n",
      "|  2|[3.0,2.0,3.0]|[3.0, 2.0, 3.0]|\n",
      "|  2|[3.0,3.0,3.0]|[3.0, 3.0, 3.0]|\n",
      "+---+-------------+---------------+\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- features_array: array (nullable = true)\n",
      " |    |-- element: float (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "\n",
    "def sum_array(v):\n",
    "  new_array = list([float(x) for x in v])\n",
    "  return new_array\n",
    "\n",
    "\n",
    "sum_array_udf = F.udf(sum_array, T.ArrayType(T.FloatType()))\n",
    "\n",
    "df11 = df.withColumn('features_array', sum_array_udf('features'))\n",
    "\n",
    "df11.show()\n",
    "df11.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+--------------+\n",
      "| id|     features|features_array|\n",
      "+---+-------------+--------------+\n",
      "|  1|[2.0,2.0,3.0]|           7.0|\n",
      "|  1|[2.0,3.0,3.0]|           8.0|\n",
      "|  2|[3.0,2.0,3.0]|           8.0|\n",
      "|  2|[3.0,3.0,3.0]|           9.0|\n",
      "+---+-------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# First define a function to do the sum map the results to float\n",
    "def sum_array(v):\n",
    "    return float(sum(v))\n",
    "\n",
    "# Define this function as a user defined function \n",
    "sum_array_udf = F.udf(sum_array, T.FloatType())\n",
    "\n",
    "# Use the withColumn operation to sum values and put it in new column \n",
    "df2=df.withColumn('features_array', sum_array_udf('features'))\n",
    "\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------------+\n",
      "| id|collect_list(features)|\n",
      "+---+----------------------+\n",
      "|  1|  [[2.0,2.0,3.0], [...|\n",
      "|  2|  [[3.0,2.0,3.0], [...|\n",
      "+---+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We can use aggregate to group data rows with the same key in groups and collect them as list. \n",
    "# We can use the function collect_list to collect. \n",
    "\n",
    "df2.groupBy(\"id\").agg(F.collect_list('features')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# newdf = df.withColumn('features_array', (df[col] for col in df.columns))\n",
    "# newdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_agg = df.agg(F.array(*[F.avg(F.col('features_array')[i]) for i in range(2)]).alias(\"averages\"))\n",
    "\n",
    "# df_agg.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec2array(v):\n",
    "  v = Vectors.dense(v)\n",
    "  array = list([float(x) for x in v])\n",
    "  return array\n",
    "\n",
    "vec2array_udf = F.udf(vec2array, T.ArrayType(T.FloatType()))\n",
    "\n",
    "# df = df.withColumn('Vec', vec2array_udf('Vec'))\n",
    "# n = len(df.select('Vec').first()[0])\n",
    "#bla = df.agg(F.array(*[F.sum(F.col(\"Vec\")[i]) for i in range(n)]).alias(\"sum\"))\n",
    "# bla.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.groupBy(\"id\").sum(\"features\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+---------------+\n",
      "| id|     features|   interactions|\n",
      "+---+-------------+---------------+\n",
      "|  1|[2.0,2.0,3.0]|[2.0, 2.0, 3.0]|\n",
      "|  1|[2.0,3.0,3.0]|[2.0, 3.0, 3.0]|\n",
      "|  2|[3.0,2.0,3.0]|[3.0, 2.0, 3.0]|\n",
      "|  2|[3.0,3.0,3.0]|[3.0, 3.0, 3.0]|\n",
      "+---+-------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "udf_to_array = udf(lambda v: [*map(float, v.toArray() )], 'array<float>')\n",
    "\n",
    "df.withColumn('interactions', udf_to_array('features')).show()\n",
    "\n",
    "# df.groupBy(\"id\").agg(\"interactions\").sum().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_agg = df.agg(F.array(*[F.avg(F.col('features_array')[i]) for i in range(10)]).alias(\"averages\"))\n",
    "# df_agg.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
